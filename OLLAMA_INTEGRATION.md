# ğŸ¤– SHCI + Ollama Integration Guide

## ğŸ“‹ Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Frontend      â”‚    â”‚   Backend       â”‚    â”‚   Ollama LLM    â”‚
â”‚   (Next.js)     â”‚â—„â”€â”€â–ºâ”‚   (FastAPI)     â”‚â—„â”€â”€â–ºâ”‚   (External)    â”‚
â”‚   Docker        â”‚    â”‚   Docker        â”‚    â”‚   Host System   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## âœ… **No Issues! Perfect Integration**

Your setup is **perfectly compatible**:
- âœ… **Backend & Frontend**: Docker containers
- âœ… **Ollama LLM**: External service (no Docker needed)
- âœ… **Communication**: Docker containers â†’ External Ollama

## ğŸ”§ **Configuration**

### **Production Server**
```bash
# Ollama (External Service)
LLM_API_URL=http://host.docker.internal:11434/v1/chat/completions
LLM_MODEL=qwen2.5-14b-gpu

# Backend (Docker Container)
CUDA_VISIBLE_DEVICES=0,1
TORCH_DEVICE=cuda
TTS_DEVICE=cuda
```

### **Local Development**
```bash
# Ollama (External Service)
LLM_API_URL=http://localhost:11434/v1/chat/completions
LLM_MODEL=qwen2.5-14b-gpu

# Backend (Docker Container)
TORCH_DEVICE=cpu
TTS_DEVICE=cpu
```

## ğŸš€ **Deployment Process**

### **1. Server Setup**
```bash
# Your Ubuntu server already has:
# âœ… Ollama running on port 11434
# âœ… qwen2.5-14b-gpu model loaded

# Deploy SHCI with Docker
git clone https://github.com/arafatkhairul/shci-app.git
cd shci-app
./deploy.sh
```

### **2. What Happens**
1. **Docker containers** start (Backend + Frontend)
2. **Backend container** connects to **external Ollama**
3. **GPU processing** for TTS/STT
4. **LLM processing** via Ollama service

## ğŸ”„ **Communication Flow**

```
User Speech â†’ Frontend â†’ Backend â†’ Ollama LLM
                â†“           â†“         â†“
            WebSocket   FastAPI   HTTP API
                â†“           â†“         â†“
            Real-time   Process   Generate
            Audio       Audio     Response
```

## ğŸ“Š **Resource Usage**

| Service | Location | GPU Usage | Memory |
|---------|----------|-----------|---------|
| **Ollama LLM** | Host System | âœ… GPU | ~8GB |
| **TTS/STT** | Docker Container | âœ… GPU | ~4GB |
| **Frontend** | Docker Container | âŒ CPU | ~1GB |
| **Backend** | Docker Container | âœ… GPU | ~2GB |

## ğŸ› ï¸ **Ollama Management**

### **Check Ollama Status**
```bash
# Check if Ollama is running
curl http://localhost:11434/api/tags

# Check loaded models
ollama list

# Check GPU usage
nvidia-smi
```

### **Ollama Commands**
```bash
# Start Ollama service
ollama serve

# Load model
ollama pull qwen2.5-14b-gpu

# Check model status
ollama show qwen2.5-14b-gpu
```

## ğŸ”§ **Troubleshooting**

### **Connection Issues**
```bash
# Test Ollama connection from container
docker-compose exec backend curl http://host.docker.internal:11434/api/tags

# Test from host
curl http://localhost:11434/api/tags
```

### **Performance Issues**
```bash
# Monitor Ollama GPU usage
watch -n 1 nvidia-smi

# Monitor Docker containers
docker stats

# Check Ollama logs
journalctl -u ollama -f
```

### **Model Loading**
```bash
# Ensure model is loaded
ollama pull qwen2.5-14b-gpu

# Check model size
du -sh ~/.ollama/models/
```

## ğŸ¯ **Benefits of This Setup**

### **âœ… Advantages**
- **Separation of Concerns**: LLM separate from TTS/STT
- **Resource Optimization**: Ollama uses GPU for LLM, Docker for TTS/STT
- **Easy Updates**: Update SHCI without affecting Ollama
- **Model Management**: Easy Ollama model switching
- **Scalability**: Can run multiple SHCI instances with one Ollama

### **ğŸ”§ Flexibility**
- **Model Switching**: Change LLM model without rebuilding containers
- **Version Control**: Update SHCI code independently
- **Resource Allocation**: Fine-tune GPU usage per service

## ğŸ“ **Environment Variables**

### **Production**
```bash
# Ollama Configuration
LLM_API_URL=http://host.docker.internal:11434/v1/chat/completions
LLM_MODEL=qwen2.5-14b-gpu
LLM_TIMEOUT=10.0
LLM_RETRIES=1

# GPU Configuration
CUDA_VISIBLE_DEVICES=0,1
TORCH_DEVICE=cuda
TTS_DEVICE=cuda
```

### **Local Development**
```bash
# Ollama Configuration
LLM_API_URL=http://localhost:11434/v1/chat/completions
LLM_MODEL=qwen2.5-14b-gpu

# CPU Configuration
TORCH_DEVICE=cpu
TTS_DEVICE=cpu
```

## ğŸš¨ **Important Notes**

1. **Ollama Must Be Running**: Ensure Ollama service is active
2. **Model Must Be Loaded**: qwen2.5-14b-gpu should be available
3. **Port Access**: Port 11434 must be accessible
4. **GPU Memory**: Ensure sufficient VRAM for both services
5. **Network**: Docker containers can access host services

## ğŸ‰ **Summary**

**Your setup is perfect!** 
- âœ… **Ollama**: External service (no Docker needed)
- âœ… **SHCI**: Docker containers (Backend + Frontend)
- âœ… **Communication**: Seamless integration
- âœ… **Performance**: GPU acceleration for both
- âœ… **Flexibility**: Easy model and service management

**No issues, ready to deploy!** ğŸš€
